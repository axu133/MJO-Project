{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baeb66d3-da8d-4617-9e48-02c7ffd98248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from netCDF4 import Dataset, num2date\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import csv\n",
    "import pandas as pd\n",
    "from timeit import default_timer\n",
    "from fourier_2d import Net2d\n",
    "\n",
    "# --------------------\n",
    "# General Setup\n",
    "# --------------------\n",
    "\n",
    "deBug = True\n",
    "seed_num = 1\n",
    "np.random.seed(seed_num)\n",
    "random.seed(seed_num)\n",
    "torch.manual_seed(seed_num)\n",
    "torch.cuda.manual_seed_all(seed_num)\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\n",
    "model_name = \"FullField\"\n",
    "lead_time_width = 2\n",
    "\n",
    "# --------------------\n",
    "# Data Directories\n",
    "# --------------------\n",
    "mdl_directory = \"/gpfs/gibbs/project/lu_lu/ax59/MJO_Project/Data/AllLeadTms/\"\n",
    "mdl_out_dir = \"Result_01/\"\n",
    "obs_directory = \"/gpfs/gibbs/project/lu_lu/ax59/MJO_Project/Data/AllLeadTms/\"\n",
    "obs_out_dir = \"Result_02/\"\n",
    "bcor_directory = \"Result_03/\"\n",
    "time_dir = \"Data/Time/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f66fdc7-0128-4a7e-bb7c-8e5ef151cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from netCDF4 import Dataset, num2date\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import csv\n",
    "import pandas as pd\n",
    "from timeit import default_timer\n",
    "from fourier_2d import Net2d\n",
    "\n",
    "# --------------------\n",
    "# General Setup\n",
    "# --------------------\n",
    "\n",
    "deBug = True\n",
    "seed_num = 1\n",
    "np.random.seed(seed_num)\n",
    "random.seed(seed_num)\n",
    "torch.manual_seed(seed_num)\n",
    "torch.cuda.manual_seed_all(seed_num)\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\n",
    "model_name = \"FullField\"\n",
    "lead_time_width = 2\n",
    "\n",
    "# --------------------\n",
    "# Data Directories\n",
    "# --------------------\n",
    "mdl_directory = \"/gpfs/gibbs/project/lu_lu/ax59/MJO_Project/Data/AllLeadTms/\"\n",
    "mdl_out_dir = \"Result_01/\"\n",
    "obs_directory = \"/gpfs/gibbs/project/lu_lu/ax59/MJO_Project/Data/AllLeadTms/\"\n",
    "obs_out_dir = \"Result_02/\"\n",
    "bcor_directory = \"Result_03/\"\n",
    "time_dir = \"Data/Time/\"\n",
    "\n",
    "# --------------------\n",
    "# Section 0: Class and Helper Function Definitions\n",
    "# --------------------\n",
    "\n",
    "class MDLDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, mdl_dir, time_dir, lead_time):\n",
    "        self.lead_time = lead_time\n",
    "        \n",
    "        # Load Data\n",
    "        variable_names = [\"TMQ\", \"FLUT\", \"U200\", \"U850\", \"TREFHT\"]\n",
    "        time_objects = None\n",
    "        lead_time_vars = []\n",
    "        \n",
    "        for i, var_name in enumerate(variable_names):\n",
    "            file_path = os.path.join(mdl_dir, f\"CML2025_Step0C_TROP30_MDL_remapped_90x180_daily_DJFM_Anom_nonFltr_{var_name}_leadTm1.nc\")\n",
    "            with Dataset(file_path) as f:\n",
    "                lead_time_vars.append(f.variables[var_name][:])\n",
    "                if i == 0:\n",
    "                    time_var = f.variables['time']\n",
    "                    time_objects = num2date(time_var[:], units=time_var.units, calendar=time_var.calendar)\n",
    "                    \n",
    "        all_features_raw = np.stack(lead_time_vars, axis=1)\n",
    "        self.times = time_objects \n",
    "        print(f\"MDL raw data loaded. {len(all_features_raw)} total entries.\")\n",
    "\n",
    "        # Boundaries of each run\n",
    "        run_boundaries = np.where(self.times[1:] < self.times[:-1])[0]\n",
    "        \n",
    "        # Create a list of start/end indices for each run\n",
    "        run_chunks_indices = []\n",
    "        start_idx = 0\n",
    "        for boundary_idx in run_boundaries:\n",
    "            end_idx = boundary_idx + 1 \n",
    "            run_chunks_indices.append((start_idx, end_idx))\n",
    "            start_idx = end_idx \n",
    "        run_chunks_indices.append((start_idx, len(all_features_raw))) \n",
    "        \n",
    "        print(f\"Found {len(run_chunks_indices)} independent runs in the file.\")\n",
    "\n",
    "        # Process runs individually\n",
    "        self.valid_pairs = []\n",
    "        for run_start, run_end in run_chunks_indices:\n",
    "            run_features = all_features_raw[run_start:run_end]\n",
    "            run_times = self.times[run_start:run_end]\n",
    "\n",
    "            # Find seasonal gaps\n",
    "            chunk_start_idx_seasonal = 0\n",
    "            # We need at least 5 days to make one sample (t-2, t-1, t, gap, t+2)\n",
    "            if len(run_times) < 5: continue \n",
    "\n",
    "            seasonal_gaps = np.where((run_times[1:] - run_times[:-1]).astype('timedelta64[D]').astype(int) > 1)[0]\n",
    "            \n",
    "            chunk_end_indices_seasonal = list(seasonal_gaps)\n",
    "            chunk_end_indices_seasonal.append(len(run_times) - 1)\n",
    "\n",
    "            for gap_idx_seasonal in chunk_end_indices_seasonal:\n",
    "                chunk_end_slice = gap_idx_seasonal + 1\n",
    "                \n",
    "                season_features = run_features[chunk_start_idx_seasonal:chunk_end_slice]\n",
    "                season_times = run_times[chunk_start_idx_seasonal:chunk_end_slice]\n",
    "                \n",
    "                # Create input-target pairs\n",
    "                num_in_season = len(season_features)\n",
    "                \n",
    "                max_start_idx = num_in_season - (2 + self.lead_time)\n",
    "                \n",
    "                for i in range(max_start_idx): \n",
    "                    # Input: concatenate t-2, t-1, t\n",
    "                    t_minus_2 = season_features[i]\n",
    "                    t_minus_1 = season_features[i+1]\n",
    "                    t_0       = season_features[i+2]\n",
    "                    \n",
    "                    feature = np.concatenate([t_minus_2, t_minus_1, t_0], axis=0)\n",
    "                    \n",
    "                    # Target: t + lead_time\n",
    "                    target_idx = i + 2 + self.lead_time\n",
    "                    target = season_features[target_idx]\n",
    "                    \n",
    "                    # Metadata for debugging\n",
    "                    feature_time = season_times[i + 2] # t\n",
    "                    target_time = season_times[target_idx]\n",
    "                    \n",
    "                    self.valid_pairs.append((feature, target, feature_time, target_time))\n",
    "                \n",
    "                chunk_start_idx_seasonal = gap_idx_seasonal + 1\n",
    "\n",
    "        print(f\"MDL dataset fully processed. Found {len(self.valid_pairs)} valid input-target pairs.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature, target, feature_time, target_time = self.valid_pairs[idx]\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(feature, dtype=torch.float32).permute(1, 2, 0),\n",
    "            torch.tensor(target, dtype=torch.float32).permute(1, 2, 0),\n",
    "        )\n",
    "\n",
    "class OBSDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, obs_dir, time_dir, lead_time, indices=None):\n",
    "        self.lead_time = lead_time\n",
    "\n",
    "        # Load Data\n",
    "        variable_names = [\"tcwv\", \"olr\", \"u200\", \"u850\", \"trefht\"]\n",
    "        time_objects = None\n",
    "        lead_time_vars = []\n",
    "        \n",
    "        for i, var_name in enumerate(variable_names):\n",
    "            file_path = os.path.join(obs_dir, f\"CML2025_Step0C_TROP30_OBS_remapped_90x180_daily_DJFM_Anom_nonFltr_{var_name}_leadTm1.nc\")\n",
    "            with Dataset(file_path) as f:\n",
    "                lead_time_vars.append(f.variables[var_name][:])\n",
    "                if i == 0:\n",
    "                    time_var = f.variables['time']\n",
    "                    time_objects = num2date(time_var[:], units=time_var.units, calendar=time_var.calendar)\n",
    "        \n",
    "        all_features_full = np.stack(lead_time_vars, axis=1)\n",
    "        all_times_full = time_objects\n",
    "\n",
    "        # Subset indices\n",
    "        if indices is not None:\n",
    "            all_features = all_features_full[indices]\n",
    "            self.times = all_times_full[indices] \n",
    "            print(f\"OBS dataset subset applied. Processing {len(all_features)} time steps for this fold.\")\n",
    "        else:\n",
    "            all_features = all_features_full\n",
    "            self.times = all_times_full\n",
    "            print(f\"OBS dataset loaded. Processing {len(all_features)} time steps.\")\n",
    "\n",
    "        # Find gaps\n",
    "        self.valid_pairs = []\n",
    "        chunk_start_idx = 0\n",
    "        \n",
    "        if len(self.times) == 0:\n",
    "            print(\"Warning: No data to process (empty indices). Dataset will be empty.\")\n",
    "            return\n",
    "\n",
    "        gaps = np.where((self.times[1:] - self.times[:-1]).astype('timedelta64[D]').astype(int) > 1)[0]\n",
    "\n",
    "        for gap_idx in np.append(gaps, len(self.times) - 1):\n",
    "            chunk_end_idx = gap_idx + 1\n",
    "            \n",
    "            chunk_features = all_features[chunk_start_idx:chunk_end_idx]\n",
    "            chunk_times = self.times[chunk_start_idx:chunk_end_idx]\n",
    "            \n",
    "            # Create input-target pairs\n",
    "            num_in_chunk = len(chunk_features)\n",
    "            \n",
    "            # Require enough frames for t-2, t-1, t ... gap ... t+lead\n",
    "            max_start_idx = num_in_chunk - (2 + self.lead_time)\n",
    "            \n",
    "            if max_start_idx > 0:\n",
    "                for i in range(max_start_idx): \n",
    "                    # Input: concatenate t-2, t-1, t\n",
    "                    t_minus_2 = chunk_features[i]\n",
    "                    t_minus_1 = chunk_features[i+1]\n",
    "                    t_0       = chunk_features[i+2]\n",
    "                    \n",
    "                    feature = np.concatenate([t_minus_2, t_minus_1, t_0], axis=0)\n",
    "                    \n",
    "                    # Target: t + lead_time\n",
    "                    target_idx = i + 2 + self.lead_time\n",
    "                    target = chunk_features[target_idx]\n",
    "                    \n",
    "                    feature_time = chunk_times[i+2]\n",
    "                    target_time = chunk_times[target_idx]\n",
    "                    \n",
    "                    self.valid_pairs.append((feature, target, feature_time, target_time))\n",
    "            \n",
    "            chunk_start_idx = chunk_end_idx\n",
    "            \n",
    "        print(f\"OBS dataset processed. Found {len(self.valid_pairs)} valid input-target pairs.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature, target, feature_time, target_time = self.valid_pairs[idx]\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(feature, dtype=torch.float32).permute(1, 2, 0),\n",
    "            torch.tensor(target, dtype=torch.float32).permute(1, 2, 0),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac7b72ee-5bc4-423e-a733-01fe5fb65fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MDL dataset... Target will be t+2 (Input: t-2, t-1, t)\n",
      "MDL raw data loaded. 43076 total entries.\n",
      "Found 8 independent runs in the file.\n",
      "MDL dataset fully processed. Found 41652 valid input-target pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ax59/.conda/envs/mjo_env_1/lib/python3.13/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "mdl_dataset = MDLDataset(mdl_dir=mdl_directory, time_dir=time_dir, lead_time=lead_time_width)\n",
    "\n",
    "# Split into training and testing sets\n",
    "mdl_total_samples = len(mdl_dataset)\n",
    "mdl_train_size = int(0.8 * mdl_total_samples) - int(0.8 * mdl_total_samples) % 121\n",
    "mdl_test_size = mdl_total_samples - mdl_train_size\n",
    "mdl_train_dataset, mdl_test_dataset = torch.utils.data.random_split(mdl_dataset, [mdl_train_size, mdl_test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 121\n",
    "train_dataloader = torch.utils.data.DataLoader(mdl_train_dataset, batch_size=batch_size, shuffle=True, pin_memory=False, num_workers=4)\n",
    "test_dataloader = torch.utils.data.DataLoader(mdl_test_dataset, batch_size=batch_size, shuffle=False, pin_memory=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e5b64d-03a4-4086-b7ec-dd006e5ad6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([121, 30, 180, 15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data, target_data = next(iter(train_dataloader))\n",
    "\n",
    "batch_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e3f0b9-5333-4c5c-be9d-33b9dfa910bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OBS dataset... Target will be t+2 (Input: t-2, t-1, t)\n",
      "OBS dataset loaded. Processing 5092 time steps.\n",
      "OBS dataset processed. Found 4924 valid input-target pairs.\n"
     ]
    }
   ],
   "source": [
    "obs_dataset = OBSDataset(obs_dir=obs_directory, time_dir=time_dir, lead_time=lead_time_width)\n",
    "total_samples = len(obs_dataset)\n",
    "\n",
    "train_test_split = 0.8\n",
    "obs_train_dataset, obs_test_dataset = torch.utils.data.random_split(obs_dataset, [int(train_test_split * total_samples), total_samples - int(train_test_split * total_samples)])\n",
    "\n",
    "batch_size = 121\n",
    "train_dataloader = torch.utils.data.DataLoader(obs_train_dataset, batch_size=batch_size, shuffle=True, pin_memory=False, num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(obs_test_dataset, batch_size=batch_size, shuffle=True, pin_memory=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a4fcdf3-7222-4a00-ae38-083ffba5210e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([121, 30, 180, 15])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data, target_data = next(iter(train_dataloader))\n",
    "\n",
    "batch_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e0cfa17-b127-4944-ac81-dcee5453aac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Performance of Untrained Model: \n",
      "Avg Train Loss: 1.1982220709096905, Avg Test Loss: 1.2055670367770623\n"
     ]
    }
   ],
   "source": [
    "modes = 12\n",
    "width = 32\n",
    "input_channels = 15\n",
    "\n",
    "model = Net2d(modes, width, in_channels=input_channels, out_channels = 5).to(device)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 500\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, threshold=0.0001, factor=0.5, mode='min')\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min = 1e-6)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Initial evaluation\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_data, batch_target in train_dataloader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "\n",
    "        y = model(batch_data)\n",
    "        loss = loss_fn(y, batch_target)\n",
    "        total_train_loss += loss.item() * batch_data.size(0)\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)\n",
    "    total_test_loss = 0.0\n",
    "\n",
    "    for batch_data, batch_target in test_dataloader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "\n",
    "        pred_y = model(batch_data)\n",
    "        loss = loss_fn(pred_y, batch_target)\n",
    "        total_test_loss += loss.item() * batch_data.size(0)\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader.dataset)\n",
    "    print(f\"Initial Performance of Untrained Model: \\nAvg Train Loss: {avg_train_loss}, Avg Test Loss: {avg_test_loss}\")\n",
    "    train_losses.append(avg_train_loss)\n",
    "    test_losses.append(avg_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425424cc-0823-4e8e-90d1-1f824afd0ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

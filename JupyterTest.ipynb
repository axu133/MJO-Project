{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "baeb66d3-da8d-4617-9e48-02c7ffd98248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from netCDF4 import Dataset, num2date\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import csv\n",
    "import pandas as pd\n",
    "from timeit import default_timer\n",
    "from fourier_2d import Net2d\n",
    "\n",
    "# --------------------\n",
    "# General Setup\n",
    "# --------------------\n",
    "\n",
    "deBug = True\n",
    "seed_num = 1\n",
    "np.random.seed(seed_num)\n",
    "random.seed(seed_num)\n",
    "torch.manual_seed(seed_num)\n",
    "torch.cuda.manual_seed_all(seed_num)\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\n",
    "model_name = \"FullField\"\n",
    "lead_time_width = 2\n",
    "\n",
    "# --------------------\n",
    "# Data Directories\n",
    "# --------------------\n",
    "mdl_directory = \"/gpfs/gibbs/project/lu_lu/ax59/MJO_Project/Data/AllLeadTms/\"\n",
    "mdl_out_dir = \"Result_01/\"\n",
    "obs_directory = \"/gpfs/gibbs/project/lu_lu/ax59/MJO_Project/Data/AllLeadTms/\"\n",
    "obs_out_dir = \"Result_02/\"\n",
    "bcor_directory = \"Result_03/\"\n",
    "time_dir = \"Data/Time/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f66fdc7-0128-4a7e-bb7c-8e5ef151cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Section 0: Class and Helper Function Definitions\n",
    "# --------------------\n",
    "\n",
    "class MDLDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, mdl_dir, time_dir, lead_time):\n",
    "        print(\"Loading MDL dataset and processing independent runs...\")\n",
    "        self.width = lead_time\n",
    "        \n",
    "        # Load Data\n",
    "        variable_names = [\"TMQ\", \"FLUT\", \"U200\", \"U850\", \"TREFHT\"]\n",
    "        time_objects = None\n",
    "        lead_time_vars = []\n",
    "        \n",
    "        for i, var_name in enumerate(variable_names):\n",
    "            file_path = os.path.join(mdl_dir, f\"CML2025_Step0C_TROP30_MDL_remapped_90x180_daily_DJFM_Anom_nonFltr_{var_name}_leadTm1.nc\")\n",
    "            with Dataset(file_path) as f:\n",
    "                lead_time_vars.append(f.variables[var_name][:])\n",
    "                if i == 0:\n",
    "                    time_var = f.variables['time']\n",
    "                    time_objects = num2date(time_var[:], units=time_var.units, calendar=time_var.calendar)\n",
    "                    \n",
    "        all_features_raw = np.stack(lead_time_vars, axis=1)\n",
    "        self.times = time_objects \n",
    "        print(f\"MDL raw data loaded. {len(all_features_raw)} total entries.\")\n",
    "\n",
    "        # Boundaries of each run\n",
    "        run_boundaries = np.where(self.times[1:] < self.times[:-1])[0]\n",
    "        \n",
    "        # Create a list of start/end indices for each run\n",
    "        run_chunks_indices = []\n",
    "        start_idx = 0\n",
    "        for boundary_idx in run_boundaries:\n",
    "            end_idx = boundary_idx + 1 # The chunk includes this index\n",
    "            run_chunks_indices.append((start_idx, end_idx))\n",
    "            start_idx = end_idx # The next chunk starts after the boundary\n",
    "        run_chunks_indices.append((start_idx, len(all_features_raw))) # Add the last run\n",
    "        \n",
    "        print(f\"Found {len(run_chunks_indices)} independent runs in the file.\")\n",
    "\n",
    "        # Process runs individually\n",
    "        self.valid_pairs = []\n",
    "        for run_start, run_end in run_chunks_indices:\n",
    "            run_features = all_features_raw[run_start:run_end]\n",
    "            run_times = self.times[run_start:run_end]\n",
    "\n",
    "            # Find gaps\n",
    "            chunk_start_idx_seasonal = 0\n",
    "            if len(run_times) < 2: continue # Skip if the run is too short\n",
    "\n",
    "            seasonal_gaps = np.where((run_times[1:] - run_times[:-1]).astype('timedelta64[D]').astype(int) > 1)[0]\n",
    "            \n",
    "            chunk_end_indices_seasonal = list(seasonal_gaps)\n",
    "            chunk_end_indices_seasonal.append(len(run_times) - 1)\n",
    "\n",
    "            for gap_idx_seasonal in chunk_end_indices_seasonal:\n",
    "                chunk_end_slice = gap_idx_seasonal + 1\n",
    "                \n",
    "                season_features = run_features[chunk_start_idx_seasonal:chunk_end_slice]\n",
    "                season_times = run_times[chunk_start_idx_seasonal:chunk_end_slice]\n",
    "                \n",
    "                # Create input-target pairs\n",
    "                num_in_season = len(season_features)\n",
    "                for i in range(num_in_season - self.width): # feature_time, target_time for debugging\n",
    "                    feature = season_features[i]\n",
    "                    target = season_features[i + self.width]\n",
    "                    feature_time = season_times[i]\n",
    "                    target_time = season_times[i + self.width]\n",
    "                    self.valid_pairs.append((feature, target, feature_time, target_time))\n",
    "                \n",
    "                chunk_start_idx_seasonal = gap_idx_seasonal + 1\n",
    "\n",
    "        print(f\"MDL dataset fully processed. Found {len(self.valid_pairs)} valid input-target pairs across all runs.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature, target, feature_time, target_time = self.valid_pairs[idx]\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(feature, dtype=torch.float32).permute(1, 2, 0),\n",
    "            torch.tensor(target, dtype=torch.float32).permute(1, 2, 0),\n",
    "        )\n",
    "\n",
    "class OBSDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, obs_dir, time_dir, lead_time, indices=None):\n",
    "        print(\"Loading OBS dataset and processing seasonal chunks...\")\n",
    "        self.width = lead_time\n",
    "\n",
    "        # Load Data\n",
    "        variable_names = [\"tcwv\", \"olr\", \"u200\", \"u850\", \"trefht\"]\n",
    "        time_objects = None\n",
    "        lead_time_vars = []\n",
    "        \n",
    "        for i, var_name in enumerate(variable_names):\n",
    "            file_path = os.path.join(obs_dir, f\"CML2025_Step0C_TROP30_OBS_remapped_90x180_daily_DJFM_Anom_nonFltr_{var_name}_leadTm1.nc\")\n",
    "            with Dataset(file_path) as f:\n",
    "                lead_time_vars.append(f.variables[var_name][:])\n",
    "                if i == 0:\n",
    "                    time_var = f.variables['time']\n",
    "                    time_objects = num2date(time_var[:], units=time_var.units, calendar=time_var.calendar)\n",
    "        \n",
    "        all_features_full = np.stack(lead_time_vars, axis=1)\n",
    "        all_times_full = time_objects\n",
    "\n",
    "        # Subset indices\n",
    "        if indices is not None:\n",
    "            all_features = all_features_full[indices]\n",
    "            self.times = all_times_full[indices] \n",
    "            print(f\"OBS dataset subset applied. Processing {len(all_features)} time steps for this fold.\")\n",
    "        else:\n",
    "            all_features = all_features_full\n",
    "            self.times = all_times_full\n",
    "            print(f\"OBS dataset loaded. Processing {len(all_features)} time steps.\")\n",
    "\n",
    "        # Find gaps\n",
    "        self.valid_pairs = []\n",
    "        chunk_start_idx = 0\n",
    "        \n",
    "        if len(self.times) == 0:\n",
    "            print(\"Warning: No data to process (empty indices). Dataset will be empty.\")\n",
    "            return\n",
    "\n",
    "        gaps = np.where((self.times[1:] - self.times[:-1]).astype('timedelta64[D]').astype(int) > 1)[0]\n",
    "\n",
    "        for gap_idx in np.append(gaps, len(self.times) - 1):\n",
    "            chunk_end_idx = gap_idx + 1\n",
    "            \n",
    "            chunk_features = all_features[chunk_start_idx:chunk_end_idx]\n",
    "            chunk_times = self.times[chunk_start_idx:chunk_end_idx]\n",
    "            \n",
    "            # Create input-target pairs\n",
    "            num_in_chunk = len(chunk_features)\n",
    "            for i in range(num_in_chunk - self.width): # feature_time, target_time for debugging\n",
    "                feature = chunk_features[i]\n",
    "                target = chunk_features[i + self.width]\n",
    "                feature_time = chunk_times[i]\n",
    "                target_time = chunk_times[i + self.width]\n",
    "                self.valid_pairs.append((feature, target, feature_time, target_time))\n",
    "            \n",
    "            chunk_start_idx = chunk_end_idx\n",
    "            \n",
    "        print(f\"OBS dataset processed. Found {len(self.valid_pairs)} valid input-target pairs.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature, target, feature_time, target_time = self.valid_pairs[idx]\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(feature, dtype=torch.float32).permute(1, 2, 0),\n",
    "            torch.tensor(target, dtype=torch.float32).permute(1, 2, 0),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac7b72ee-5bc4-423e-a733-01fe5fb65fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MDL dataset and processing independent runs...\n",
      "MDL raw data loaded. 43076 total entries.\n",
      "Found 8 independent runs in the file.\n",
      "MDL dataset fully processed. Found 42364 valid input-target pairs across all runs.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "mdl_dataset = MDLDataset(mdl_dir=mdl_directory, time_dir=time_dir, lead_time=lead_time_width)\n",
    "\n",
    "# Split into training and testing sets\n",
    "mdl_total_samples = len(mdl_dataset)\n",
    "mdl_train_size = int(0.8 * mdl_total_samples) - int(0.8 * mdl_total_samples) % 121\n",
    "mdl_test_size = mdl_total_samples - mdl_train_size\n",
    "mdl_train_dataset, mdl_test_dataset = torch.utils.data.random_split(mdl_dataset, [mdl_train_size, mdl_test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 121\n",
    "train_dataloader = torch.utils.data.DataLoader(mdl_train_dataset, batch_size=batch_size, shuffle=True, pin_memory=False, num_workers=4)\n",
    "test_dataloader = torch.utils.data.DataLoader(mdl_test_dataset, batch_size=batch_size, shuffle=False, pin_memory=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25e5b64d-03a4-4086-b7ec-dd006e5ad6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([121, 30, 180, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data, target_data = next(iter(train_dataloader))\n",
    "\n",
    "batch_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53e3f0b9-5333-4c5c-be9d-33b9dfa910bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OBS dataset and processing seasonal chunks...\n",
      "OBS dataset loaded. Processing 5092 time steps.\n",
      "OBS dataset processed. Found 5008 valid input-target pairs.\n"
     ]
    }
   ],
   "source": [
    "obs_dataset = OBSDataset(obs_dir=obs_directory, time_dir=time_dir, lead_time=lead_time_width)\n",
    "total_samples = len(obs_dataset)\n",
    "\n",
    "train_test_split = 0.8\n",
    "obs_train_dataset, obs_test_dataset = torch.utils.data.random_split(obs_dataset, [int(train_test_split * total_samples), total_samples - int(train_test_split * total_samples)])\n",
    "\n",
    "batch_size = 121\n",
    "train_dataloader = torch.utils.data.DataLoader(obs_train_dataset, batch_size=batch_size, shuffle=True, pin_memory=False, num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(obs_test_dataset, batch_size=batch_size, shuffle=True, pin_memory=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a4fcdf3-7222-4a00-ae38-083ffba5210e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([121, 30, 180, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data, target_data = next(iter(train_dataloader))\n",
    "\n",
    "batch_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e0cfa17-b127-4944-ac81-dcee5453aac4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Net2d.__init__() got an unexpected keyword argument 'in_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m width = \u001b[32m32\u001b[39m\n\u001b[32m      3\u001b[39m input_channels = \u001b[32m5\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mNet2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_channels\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      7\u001b[39m lr = \u001b[32m0.001\u001b[39m\n\u001b[32m      8\u001b[39m epochs = \u001b[32m500\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Net2d.__init__() got an unexpected keyword argument 'in_channels'"
     ]
    }
   ],
   "source": [
    "modes = 12\n",
    "width = 32\n",
    "input_channels = 5\n",
    "\n",
    "model = Net2d(modes, width, in_channels=input_channels).to(device)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 500\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, threshold=0.0001, factor=0.5, mode='min')\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min = 1e-6)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Initial evaluation\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_data, batch_target in train_dataloader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "\n",
    "        y = model(batch_data)\n",
    "        loss = loss_fn(y, batch_target)\n",
    "        total_train_loss += loss.item() * batch_data.size(0)\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)\n",
    "    total_test_loss = 0.0\n",
    "\n",
    "    for batch_data, batch_target in test_dataloader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "\n",
    "        pred_y = model(batch_data)\n",
    "        loss = loss_fn(pred_y, batch_target)\n",
    "        total_test_loss += loss.item() * batch_data.size(0)\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader.dataset)\n",
    "    print(f\"Initial Performance of Untrained Model: \\nAvg Train Loss: {avg_train_loss}, Avg Test Loss: {avg_test_loss}\")\n",
    "    train_losses.append(avg_train_loss)\n",
    "    test_losses.append(avg_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425424cc-0823-4e8e-90d1-1f824afd0ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
